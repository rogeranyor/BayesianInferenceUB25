{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02397829",
   "metadata": {},
   "source": [
    "\n",
    "### What is A/B Testing?\n",
    "\n",
    "A/B testing (aka split testing) is a controlled experiment comparing two (or more) variants, A (control) vs B (treatment), to see which performs better on a defined metric. It is essentially a randomized controlled trial (RCT) applied to user experiences – a concept dating back to R.A. Fisher’s experiments in the 1920s. Random assignment ensures groups are identical except for treatment, enabling causal conclusions. A/B tests are “the bread and butter of most tech companies”; major firms like Google and Microsoft run over 10,000 annually.\n",
    "\n",
    "### Why is A/B Testing Important?\n",
    "\n",
    "A/B testing enables data-driven decisions by empirically measuring the impact of changes. It's a cornerstone of evidence-based practice in fields from tech to public health. For example, a minor ad headline test at Bing increased revenue by 12% (over $100M/year), a result discovered via A/B testing that wouldn’t have been prioritized otherwise.\n",
    "\n",
    "### How to Design and Run a Classical A/B Test\n",
    "\n",
    "1. **Define Objective and Metric:** Formulate a hypothesis and select a measurable metric (e.g., conversion rate).\n",
    "2. **Plan the Experiment:** Randomly assign users (typically 50/50), and perform power analysis to determine sample size.\n",
    "3. **Randomize and Run:** Assign users to A or B and collect outcome data.\n",
    "4. **Analyze Outcomes:** Choose statistical test based on data type:\n",
    "   - Binary outcomes: z-test for proportions\n",
    "   - Continuous outcomes: two-sample t-test\n",
    "   - Non-normal: Mann-Whitney U\n",
    "   Test null hypothesis H₀ (no effect), compute p-value, and check significance.\n",
    "5. **Interpret Results:** Use p-value and confidence interval to evaluate results. Remember: statistical ≠ practical significance.\n",
    "6. **Decide and Iterate:** Roll out changes or iterate based on results.\n",
    "\n",
    "### Example Case Studies\n",
    "\n",
    "- **Bing Ad Test:** A simple layout change increased revenue by 12%.\n",
    "- **Obama Campaign:** Email design change improved sign-up conversion by ~40%, adding 4M emails and $75M in donations.\n",
    "- **Healthcare Reminder:** Reminders increased mammogram rates from 29.5% to 48.6%.\n",
    "- **Cookie Cats Game:** Changed level gating; no significant effect on retention.\n",
    "\n",
    "### Discussion – Pros and Pitfalls\n",
    "\n",
    "**Strengths:** Simple, causal by design, well-established.\n",
    "\n",
    "**Pitfalls:**\n",
    "- Peeking inflates Type I error.\n",
    "- Misinterpreting p-values.\n",
    "- False positives from multiple testing.\n",
    "- Underpowered studies yield false negatives.\n",
    "- Heterogeneous effects obscured in averages.\n",
    "- Short-term vs long-term effects.\n",
    "- Statistical significance ≠ practical impact.\n",
    "\n",
    "Classical A/B testing is powerful but requires rigor in design and interpretation. These challenges set the stage for Bayesian methods, which we explore next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5df0c",
   "metadata": {},
   "source": [
    "### Example Case Studies (Classical A/B Testing)\n",
    "\n",
    "To ground these concepts, we discuss a few real-world A/B tests:\n",
    "\n",
    "#### Tech (Search Engine): Bing Ad Headline Experiment\n",
    "An engineer at Microsoft Bing tested a new way of displaying ad titles. The A/B test showed a **+12% lift in revenue** for the new variant within hours, a remarkably large effect. The result was highly significant (triggering an alert that usually indicates a bug because of the magnitude). This led to an immediate implementation of the change, yielding **over $100M in annual revenue** – a testament to how A/B testing can uncover “low-hanging fruit” that intuition might overlook.  \n",
    "(*Source: hbr.org*)\n",
    "\n",
    "#### Tech/Social (Political Campaign): Obama Campaign Email Experiment\n",
    "During the 2008 U.S. presidential campaign, the Obama digital team obsessively ran A/B tests on their website and emails. In one test, they tried different headline text and media on the campaign’s email signup splash page. Surprisingly, a simple “Learn More” button with a family photo outperformed the original “Sign Up” button with a splashy video – increasing sign-up conversions by **~40%**. Staff had expected the opposite, proving that intuition can be wrong. The winning variant helped gather **4 million additional email subscribers** and raise an extra **$75 million** in donations.  \n",
    "(*Source: wired.com*)\n",
    "\n",
    "#### Healthcare\n",
    "In medicine, A/B testing takes the form of randomized trials. A study tested sending **electronic reminders** to patients to improve preventive care. Patients were randomly assigned to either receive reminders via an online portal or not. The results showed significantly higher uptake of screenings in the intervention group – e.g., **48.6% of women got a mammogram with reminders vs 29.5% without** (~19 percentage-point increase). Such randomized trials are critical for evaluating interventions, though ethical constraints can sometimes limit randomization.  \n",
    "(*Source: pubmed.ncbi.nlm.nih.gov*)\n",
    "\n",
    "#### Online Gaming: Cookie Cats\n",
    "The mobile game Cookie Cats ran an A/B test on game progression design. Variant A had a “gate” at level 30, and variant B moved the gate to level 40. **90,000+ players** were split between the two. One-day retention was 44.8% (A) vs 44.2% (B); 7-day retention was 19.0% (A) vs 18.2% (B). The small differences favored the original gate, but statistical tests showed **no significant difference at p<0.05**. The team concluded the change had no meaningful benefit. This highlights the value of testing: even null results can prevent poor decisions.  \n",
    "(*Source: medium.com*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d7f01",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
